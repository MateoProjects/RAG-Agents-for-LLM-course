{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 3:** Running State Chains</font>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio\n",
    "\n",
    "## If you're in colab and encounter a typing-extensions issue,\n",
    "##  restart your runtime and try again\n",
    "# from langchain_nvidia_ai_endpoints._common import NVEModel\n",
    "from getpass import getpass\n",
    "import requests\n",
    "import os\n",
    "\n",
    "hard_reset = False  ## <-- Set to True if you want to reset your NVIDIA_API_KEY\n",
    "while \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\") or hard_reset:\n",
    "    try: \n",
    "        response = requests.get(\"http://docker_router:8070/get_key\").json()\n",
    "        assert response.get('nvapi_key')\n",
    "    except: response = {'nvapi_key' : getpass(\"NVIDIA API Key: \")}\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = response.get(\"nvapi_key\")\n",
    "    try: requests.post(\"http://docker_router:8070/set_key/\", json={'nvapi_key' : os.environ[\"NVIDIA_API_KEY\"]}).json()\n",
    "    except: pass\n",
    "    hard_reset = False\n",
    "    if \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\"):\n",
    "        print(\"[!] API key assignment failed. Make sure it starts with `nvapi-` as generated from the model pages.\")\n",
    "\n",
    "print(f\"Retrieved NVIDIA_API_KEY beginning with \\\"{os.environ.get('NVIDIA_API_KEY')[:9]}...\\\"\")\n",
    "from langchain_nvidia_ai_endpoints._common import NVEModel\n",
    "NVEModel().available_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('nvidia_api_key.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Accede a la clave \"api\" en los datos cargados\n",
    "nvidia_api_key = json_data[\"api\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from copy import deepcopy\n",
    "\n",
    "inst_llm = ChatNVIDIA(model=\"mixtral_8x7b\")  ## Feel free to change the models\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Only respond in rhymes\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "prompt2 =  ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"Only responding in rhyme, change the topic of the input poem to be about {topic}!\"\n",
    "        \" Make it happy! Try to keep the same sentence structure, but make sure it's easy to recite!\"\n",
    "        \" Try not to rhyme a word with itself.\"\n",
    "    )),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "## These are the main chains, constructed here as modules of functionality.\n",
    "chain1 = prompt1 | inst_llm | StrOutputParser()  ## only expects input\n",
    "chain2 = prompt2 | inst_llm | StrOutputParser()  ## expects both input and topic\n",
    "\n",
    "################################################################################\n",
    "## SUMMARY OF TASK: chain1 currently gets invoked for the first input.\n",
    "##  Please invoke chain2 for subsequent invocations.\n",
    "\n",
    "def rhyme_chat2_stream(message, history, return_buffer=True):\n",
    "    '''This is a generator function, where each call will yield the next entry'''\n",
    "\n",
    "    first_poem = None\n",
    "    for entry in history:\n",
    "        if entry[0] and entry[1]:\n",
    "            ## If a generation occurred as a direct result of a user input,\n",
    "            ##  keep that response (the first poem generated) and break out\n",
    "            first_poem = entry[1]\n",
    "            break\n",
    "\n",
    "    if first_poem is None:\n",
    "        ## First Case: There is no initial poem generated. Better make one up!\n",
    "\n",
    "        buffer = \"Oh! I can make a wonderful poem about that! Let me think!\\n\\n\"\n",
    "        yield buffer\n",
    "\n",
    "        ## iterate over stream generator for first generation\n",
    "        inst_out = \"\"\n",
    "        chat_gen = chain1.stream({\"input\" : message})\n",
    "        for token in chat_gen:\n",
    "            inst_out += token\n",
    "            buffer += token\n",
    "            yield buffer if return_buffer else token\n",
    "\n",
    "        passage = \"\\n\\nNow let me rewrite it with a different focus! What should the new focus be?\"\n",
    "        buffer += passage\n",
    "        yield buffer if return_buffer else passage\n",
    "\n",
    "    else:\n",
    "        ## Subsequent Cases: There is a poem to start with. Generate a similar one with a new topic!\n",
    "\n",
    "        buffer = f\"Sure! Here you go!\\n\\n\"\n",
    "        yield buffer\n",
    "\n",
    "        chat_gen = chain2.stream({\"input\" : first_poem, \"topic\" : message})\n",
    "        for token in chat_gen:\n",
    "            buffer += token\n",
    "            yield buffer if return_buffer else token\n",
    "\n",
    "        passage = \"\\n\\nThis is fun! Give me another topic!\"\n",
    "        buffer += passage\n",
    "        yield buffer if return_buffer else passage\n",
    "\n",
    "################################################################################\n",
    "## Below: This is a small-scale simulation of the gradio routine.\n",
    "\n",
    "def queue_fake_streaming_gradio(chat_stream, history = [], max_questions=5):\n",
    "\n",
    "    ## Mimic of the gradio initialization routine, where a set of starter messages can be printed off\n",
    "    for human_msg, agent_msg in history:\n",
    "        if human_msg: print(\"\\n[ Human ]:\", human_msg)\n",
    "        if agent_msg: print(\"\\n[ Agent ]:\", agent_msg)\n",
    "\n",
    "    ## Mimic of the gradio loop with an initial message from the agent.\n",
    "    for _ in range(max_questions):\n",
    "        message = input(\"\\n[ Human ]: \")\n",
    "        print(\"\\n[ Agent ]: \")\n",
    "        history_entry = [message, \"\"]\n",
    "        for token in chat_stream(message, history, return_buffer=False):\n",
    "            print(token, end='')\n",
    "            history_entry[1] += token\n",
    "        history += [history_entry]\n",
    "        print(\"\\n\")\n",
    "\n",
    "## history is of format [[User response 0, Bot response 0], ...]\n",
    "history = [[None, \"Let me help you make a poem! What would you like for me to write?\"]]\n",
    "\n",
    "## Simulating the queueing of a streaming gradio interface, using python input\n",
    "queue_fake_streaming_gradio(\n",
    "    chat_stream = rhyme_chat2_stream,\n",
    "    history = history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableMap,       ## Wrap an implicit \"dictionary\" runnable\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, ChatMessage, AIMessage\n",
    "from typing import Iterable\n",
    "import gradio as gr\n",
    "\n",
    "external_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a SkyFlow chatbot, and you are helping a customer with their issue. Please chat with them! Stay concise and clear!\"\n",
    "        \" Your running knowledge base is: {know_base}. This is for you only; Do not mention it! \\nUsing that, we retrieved the following: {context}\\n\"\n",
    "        \" If they provide info and the retrieval fails, ask to confirm their first/last name and confirmation. Do not ask them any other personal info.\"\n",
    "        \" If it's not important to know about their flight, do not ask. The checking happens automatically; you cannot check manually.\"\n",
    "    )),\n",
    "    (\"assistant\", \"{output}\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "##########################################################################\n",
    "## Knowledge Base Things\n",
    "\n",
    "class KnowledgeBase(BaseModel):\n",
    "    first_name: str = Field('unknown', description=\"Chatting user's first name, `unknown` if unknown\")\n",
    "    last_name: str = Field('unknown', description=\"Chatting user's last name, `unknown` if unknown\")\n",
    "    confirmation: Optional[int] = Field(None, description=\"Flight Confirmation Number, `-1` if unknown\")\n",
    "    discussion_summary: str = Field(\"\", description=\"Summary of discussion so far, including locations, issues, etc.\")\n",
    "    open_problems: str = Field(\"\", description=\"Topics that have not been resolved yet\")\n",
    "    current_goals: str = Field(\"\", description=\"Current goal for the agent to address\")\n",
    "\n",
    "parser_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', (\n",
    "        \"You are a chat assistant representing the airline SkyFlow, and are trying to track info about the conversation. \"\n",
    "        \"You have just recieved a message from the user. Please fill in the schema based on the chat. \\n{format_instructions}\"\n",
    "    )), ('user', \"{know_base} is the current state. New exchange: You said {output} and they replied {input}\")\n",
    "])\n",
    "\n",
    "fail_str = (\n",
    "    \"You cannot access user's flight/account details until they provide \"\n",
    "    \"first name, last name, and flight confirmation code.\"\n",
    ")\n",
    "\n",
    "## Your goal is to invoke the following through natural conversation\n",
    "# get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}) ->\n",
    "#     \"Jane Doe's flight from San Jose to New Orleans departs at 12:30 PM tomorrow and lands at 9:30 PM.\"\n",
    "\n",
    "## TODO: Pick a working model that you're happy with\n",
    "instruct_llm = (ChatNVIDIA(model=\"mixtral_8x7b\") | StrOutputParser())\n",
    "chat_model = (ChatNVIDIA(model=\"mixtral_8x7b\") | StrOutputParser())\n",
    "\n",
    "## The external chain is already supported, and just streams the output\n",
    "external_chain = external_prompt | chat_model\n",
    "\n",
    "## The internal chain is invoked in the loop, but is not defined yet\n",
    "\n",
    "#####################################################################################\n",
    "## START TODO: Define the extractor and internal chain to satisfy the objective\n",
    "\n",
    "extractor = RunnableAssign({'know_base' : RExtract(KnowledgeBase, instruct_llm, parser_prompt)})\n",
    "internal_chain = extractor | RunnableAssign(\n",
    "    {'context' : lambda d: itemgetter('know_base') | get_key | get_flight_info}\n",
    ")\n",
    "\n",
    "## END TODO\n",
    "#####################################################################################\n",
    "\n",
    "state = {'know_base' : KnowledgeBase()}\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=False):\n",
    "\n",
    "    ## Pulling in, updating, and printing the state\n",
    "    global state\n",
    "    state['input'] = message\n",
    "    state['history'] = history\n",
    "    state['output'] = \"\" if not history else history[-1][1]\n",
    "\n",
    "    ## Generating the new state from the internal chain\n",
    "    state = internal_chain.invoke(state)\n",
    "    print(\"State after chain run:\", state)\n",
    "\n",
    "    ## Streaming the results\n",
    "    buffer = \"\"\n",
    "    for token in external_chain.stream(state):\n",
    "        buffer += token\n",
    "        yield buffer\n",
    "\n",
    "chatbot = gr.Chatbot(value = [[None, \"Hello! I'm your SkyFlow agent! How can I help you?\"]])\n",
    "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "try:\n",
    "    ## NOTE: This should also give you a temporary public link which can be\n",
    "    ## used to access this info on the public web while the session is live.\n",
    "    demo.launch(debug=True, share=True, show_api=False)\n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
