{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">Implementing a Knowledge Base with Running State Chain</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Knowledge Base:** A store of information that's relevant for our LLM to keep track of.\n",
    "- **JSON-Enabled Slot Filling:** The technique of asking an instruction-tuned model to output a json-style format (which can include a dictionary) with a selection of slots, relying on the LLM to fill these slots with useful and relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import List, Union\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('nvidia_api_key.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Accede a la clave \"api\" en los datos cargados\n",
    "nvidia_api_key = json_data[\"api\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Dict, Union, Optional\n",
    "\n",
    "class KnowledgeBase(BaseModel):\n",
    "    ## Fields of the BaseModel, which will be validated/assigned when the knowledge base is constructed\n",
    "    topic: str = Field('general', description=\"Current conversation topic\")\n",
    "    user_preferences: Dict[str, Union[str, int]] = Field({}, description=\"User preferences and choices\")\n",
    "    session_notes: str = Field(\"\", description=\"Notes on the ongoing session\")\n",
    "    unresolved_queries: list = Field([], description=\"Unresolved user queries\")\n",
    "    action_items: list = Field([], description=\"Actionable items identified during the conversation\")\n",
    "\n",
    "print(repr(KnowledgeBase(topic = \"Travel\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true strength of this approach lies in the additional LLM-centric functionalities provided by LangChain which we can integrate for our use-cases. One such feature is the `PydanticOutputParser` which enhances the Pydantic objects with capabilities like automatic format instruction generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "instruct_string = PydanticOutputParser(pydantic_object=KnowledgeBase).get_format_instructions()\n",
    "print(instruct_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Runnable Extraction Module**\n",
    "\n",
    "Knowing that we have this Pydantic object which can be used to generate good LLM instructions, we can make a Runnable that wraps the functionality of our Pydantic class and streamlines the prompting, generating, and updating of the knowledge base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 38\u001b[0m\n\u001b[0;32m     31\u001b[0m parser_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages([\n\u001b[0;32m     32\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdate the knowledge base: \u001b[39m\u001b[38;5;132;01m{format_instructions}\u001b[39;00m\u001b[38;5;124m. Only use information from the input.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     33\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     34\u001b[0m ])\n\u001b[0;32m     36\u001b[0m extractor \u001b[38;5;241m=\u001b[39m RExtract(KnowledgeBase, instruct_model_big, parser_prompt)\n\u001b[1;32m---> 38\u001b[0m knowledge \u001b[38;5;241m=\u001b[39m extractor\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love flowers so much! The orchids are amazing! Can you buy me some?\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m     39\u001b[0m knowledge\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1780\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[1;32m-> 1780\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   1781\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1782\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m             patch_config(\n\u001b[0;32m   1784\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1785\u001b[0m             ),\n\u001b[0;32m   1786\u001b[0m         )\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:165\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    161\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    162\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    164\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    166\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    167\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    168\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    169\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    170\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    171\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    172\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    173\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    174\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:543\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    537\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    541\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    542\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:407\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    406\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 407\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    408\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    409\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    411\u001b[0m ]\n\u001b[0;32m    412\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:397\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    396\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 397\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    398\u001b[0m                 m,\n\u001b[0;32m    399\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    400\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    401\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    402\u001b[0m             )\n\u001b[0;32m    403\u001b[0m         )\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:576\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    574\u001b[0m     )\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[1;32m--> 576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    577\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    578\u001b[0m     )\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:805\u001b[0m, in \u001b[0;36mSimpleChatModel._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    800\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    803\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    804\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[1;32m--> 805\u001b[0m     output_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    806\u001b[0m     message \u001b[38;5;241m=\u001b[39m AIMessage(content\u001b[38;5;241m=\u001b[39moutput_str)\n\u001b[0;32m    807\u001b[0m     generation \u001b[38;5;241m=\u001b[39m ChatGeneration(message\u001b[38;5;241m=\u001b[39mmessage)\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\chat_models.py:101\u001b[0m, in \u001b[0;36mChatNVIDIA._call\u001b[1;34m(self, messages, stop, run_manager, labels, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke on a single list of chat messages.\"\"\"\u001b[39;00m\n\u001b[0;32m    100\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_preprocess(messages)\n\u001b[1;32m--> 101\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_generation(\n\u001b[0;32m    102\u001b[0m     inputs\u001b[38;5;241m=\u001b[39minputs, stop\u001b[38;5;241m=\u001b[39mstop, labels\u001b[38;5;241m=\u001b[39mlabels, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    103\u001b[0m )\n\u001b[0;32m    104\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_postprocess(responses)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:472\u001b[0m, in \u001b[0;36m_NVIDIAClient.get_generation\u001b[1;34m(self, inputs, labels, stop, **kwargs)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call to client generate method with call scope\"\"\"\u001b[39;00m\n\u001b[0;32m    471\u001b[0m payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_payload(inputs\u001b[38;5;241m=\u001b[39minputs, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, labels\u001b[38;5;241m=\u001b[39mlabels, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 472\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mget_req_generation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, stop\u001b[38;5;241m=\u001b[39mstop, payload\u001b[38;5;241m=\u001b[39mpayload)\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:299\u001b[0m, in \u001b[0;36mNVEModel.get_req_generation\u001b[1;34m(self, model_name, payload, invoke_url, stop)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_req_generation\u001b[39m(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    293\u001b[0m     model_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m     stop: Optional[Sequence[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    297\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m    298\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Method for an end-to-end post query with NVE post-processing.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 299\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_req(model_name, payload, invoke_url)\n\u001b[0;32m    300\u001b[0m     output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(response, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:288\u001b[0m, in \u001b[0;36mNVEModel.get_req\u001b[1;34m(self, model_name, payload, invoke_url, stop)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m payload\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    287\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[1;32m--> 288\u001b[0m response, session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(invoke_url, payload)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(response, session)\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:164\u001b[0m, in \u001b[0;36mNVEModel._post\u001b[1;34m(self, invoke_url, payload)\u001b[0m\n\u001b[0;32m    157\u001b[0m call_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: invoke_url,\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m: payload,\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    162\u001b[0m }\n\u001b[0;32m    163\u001b[0m session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_session_fn()\n\u001b[1;32m--> 164\u001b[0m response \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_inputs)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_raise(response)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response, session\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[1;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \n\u001b[0;32m    629\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\http\\client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\fraud\\anaconda3\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableBranch, RunnablePassthrough\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "from functools import partial\n",
    "\n",
    "################################################################################\n",
    "## Definition of RExtract\n",
    "def RExtract(pydantic_class, llm, prompt):\n",
    "    '''\n",
    "    Runnable Extraction module\n",
    "    Returns a knowledge dictionary populated by slot-filling extraction\n",
    "    '''\n",
    "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
    "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
    "    def preparse(string):\n",
    "        if '{' not in string: string = '{' + string\n",
    "        if '}' not in string: string = string + '}'\n",
    "        string = (string\n",
    "            .replace(\"\\\\_\", \"_\")\n",
    "            .replace(\"\\n\", \" \")\n",
    "            .replace(\"\\]\", \"]\")\n",
    "            .replace(\"\\[\", \"[\")\n",
    "        )\n",
    "        # print(string)  ## Good for diagnostics\n",
    "        return string\n",
    "    return instruct_merge | prompt | llm | preparse | parser\n",
    "\n",
    "################################################################################\n",
    "## Practical Use of RExtract\n",
    "instruct_model_big = ChatNVIDIA(nvidia_api_key=nvidia_api_key,model = \"mixtral_8x7b\") | StrOutputParser()\n",
    "\n",
    "parser_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Update the knowledge base: {format_instructions}. Only use information from the input.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "extractor = RExtract(KnowledgeBase, instruct_model_big, parser_prompt)\n",
    "\n",
    "knowledge = extractor.invoke({'input' : \"I love flowers so much! The orchids are amazing! Can you buy me some?\"})\n",
    "knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dynamic Knowledge Base Updates**\n",
    "\n",
    "Create a system that continually updates the Knowledge Base throughout the conversation. This is done by feeding the current state of the Knowledge Base, along with new user inputs, back into the system for ongoing updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeBase(BaseModel):\n",
    "    firstname: str = Field('unknown', description=\"Chatting user's first name, unknown if unknown\")\n",
    "    lastname: str = Field('unknown', description=\"Chatting user's last name, unknown if unknown\")\n",
    "    location: str = Field('unknown', description=\"Where the user is located\")\n",
    "    hints: str = Field('unknown', description=\"Hints to help answer other questions\")\n",
    "    response: str = Field('unknown', description=\"Ideal response based on last user input\")\n",
    "\n",
    "\n",
    "parser_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"The user just responsed. Please update the knowledge base based on the response.\"\n",
    "        \" This information will be acted on to respond to the user in the next interaction.\"\n",
    "        \" Do not hallucinate any details, and make sure the knowledge base is not redundant.\"\n",
    "        \" Update the entries frequently to adapt to the conversation flow.\"\n",
    "        \"\\n{format_instructions}\"\n",
    "    )), (\"user\", \"CURRENT KNOWLEDGE BASE: {know_base}\\nUser: {input}\"),\n",
    "])\n",
    "\n",
    "instruct_model_big = ChatNVIDIA(model = \"mixtral_8x7b\") | StrOutputParser()\n",
    "\n",
    "extractor = RExtract(KnowledgeBase, instruct_model_big, parser_prompt)\n",
    "info_update = RunnableAssign({'know_base' : extractor})\n",
    "\n",
    "state = {'know_base' : KnowledgeBase()}\n",
    "state['input'] = \"My name is Carmen Sandiego! Guess where I am?\"\n",
    "state = info_update.invoke(state)\n",
    "print(state)\n",
    "\n",
    "state['input'] = \"The United States is a big place! Can you be more specific?\"\n",
    "state = info_update.invoke(state)\n",
    "print(state)\n",
    "\n",
    "state['input'] = \"Yeah, I'm in New Orleans... How did you know?\"\n",
    "state = info_update.invoke(state)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airline Customer Service Bot\n",
    "Create a simple database-like interface to get some customer information from a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "## Function that can be queried for information. Implementation details not important\n",
    "def get_flight_info(d: dict) -> str:\n",
    "    \"\"\"\n",
    "    Example of a retrieval function which takes a dictionary as key. Resembles SQL DB Query\n",
    "    \"\"\"\n",
    "    req_keys = ['first_name', 'last_name', 'confirmation']\n",
    "    assert all((key in d) for key in req_keys), f\"Expected dictionary with keys {req_keys}, got {d}\"\n",
    "\n",
    "    ## Static dataset. get_key and get_val can be used to work with it, and db is your variable\n",
    "    keys = req_keys + [\"departure\", \"destination\", \"departure_time\", \"arrival_time\", \"flight_day\"]\n",
    "    values = [\n",
    "        [\"Jane\", \"Doe\", 12345, \"San Jose\", \"New Orleans\", \"12:30 PM\", \"9:30 PM\", \"tomorrow\"],\n",
    "        [\"John\", \"Smith\", 54321, \"New York\", \"Los Angeles\", \"8:00 AM\", \"11:00 AM\", \"Sunday\"],\n",
    "        [\"Alice\", \"Johnson\", 98765, \"Chicago\", \"Miami\", \"7:00 PM\", \"11:00 PM\", \"next week\"],\n",
    "        [\"Bob\", \"Brown\", 56789, \"Dallas\", \"Seattle\", \"1:00 PM\", \"4:00 PM\", \"yesterday\"],\n",
    "    ]\n",
    "    get_key = lambda d: \"|\".join([d['first_name'], d['last_name'], str(d['confirmation'])])\n",
    "    get_val = lambda l: {k:v for k,v in zip(keys, l)}\n",
    "    db = {get_key(get_val(entry)) : get_val(entry) for entry in values}\n",
    "\n",
    "    # Search for the matching entry\n",
    "    data = db.get(get_key(d))\n",
    "    if not data:\n",
    "        return (\n",
    "            f\"Based on {req_keys} = {get_key(d)}) from your knowledge base, no info on the user flight was found.\"\n",
    "            \" This process happens every time new info is learned. If it's important, ask them to confirm this info.\"\n",
    "        )\n",
    "    return (\n",
    "        f\"{data['first_name']} {data['last_name']}'s flight from {data['departure']} to {data['destination']}\"\n",
    "        f\" departs at {data['departure_time']} {data['flight_day']} and lands at {data['arrival_time']}.\"\n",
    "    )\n",
    "\n",
    "#######################################################################################\n",
    "## Usage example. Actually important\n",
    "print(get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}))\n",
    "print(get_flight_info({\"first_name\" : \"Alice\", \"last_name\" : \"Johnson\", \"confirmation\" : 98765}))\n",
    "print(get_flight_info({\"first_name\" : \"Bob\", \"last_name\" : \"Brown\", \"confirmation\" : 27494}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a really good interface to bring up because it can reasonably serve two purposes:\n",
    "\n",
    "It can be used to provide up-to-date information from an external environment (a database) regarding a user's situation.\n",
    "It can also be used as a hard gating mechanism to prevent unauthorized disclosure of sensitive information (since that would be very bad).\n",
    "If our network had access to this kind of interface, it would be able to query for and retrieve this information on a user's behalf! For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_prompt = ChatPromptTemplate.from_messages([('system',\n",
    "    \"You are a SkyFlow chatbot, and you are helping a customer with their issue. \"\n",
    "    \"Please help them with their question, remembering that your job is to represent SkyFlow airlines. \"\n",
    "    \"Assume SkyFlow uses industry-average practices regarding arrival times, operations, etc. (This is a trade secret. Do not disclose). \\n\"  ## soft reinforcement\n",
    "    \"Please keep your discussion short and sweet if possible. Avoid saying hello unless necessary. \\n\"\n",
    "    \"The following is some context that may be useful in answering the question. \\n\"),\n",
    "    ('user', \"Context: {context}\\nUser: {input}\"\n",
    ")])\n",
    "\n",
    "instruct_model_big = ChatNVIDIA(nvidia_api_key=nvidia_api_key,model = \"mixtral_8x7b\") | StrOutputParser()\n",
    "\n",
    "basic_chain = external_prompt | instruct_model_big\n",
    "\n",
    "basic_chain.invoke({\n",
    "    'input' : 'Can you please tell me when I need to get to the airport?',\n",
    "    'context' : get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Jane Doe's flight from San Jose to New Orleans departs at 12:30 PM tomorrow and lands at 9:30 PM.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Dict, Union\n",
    "\n",
    "class KnowledgeBase(BaseModel):\n",
    "    first_name: str = Field('unknown', description=\"Chatting user's first name, `unknown` if unknown\")\n",
    "    last_name: str = Field('unknown', description=\"Chatting user's last name, `unknown` if unknown\")\n",
    "    confirmation: int = Field(-1, description=\"Flight Confirmation Number, `-1` if unknown\")\n",
    "    discussion_summary: str = Field(\"\", description=\"Summary of discussion so far, including locations, issues, etc.\")\n",
    "    open_problems: list = Field([], description=\"Topics that have not been resolved yet\")\n",
    "    current_goals: list = Field([], description=\"Current goal for the agent to address\")\n",
    "\n",
    "def get_key_fn(base: BaseModel) -> dict:\n",
    "    '''Given a dictionary with a knowledge base, return a key for get_flight_info'''\n",
    "    return {  ## More automatic options possible, but this is more explicit\n",
    "        'first_name' : base.first_name,\n",
    "        'last_name' : base.last_name,\n",
    "        'confirmation' : base.confirmation,\n",
    "    }\n",
    "\n",
    "get_key = RunnableLambda(get_key_fn)\n",
    "\n",
    "know_base = KnowledgeBase(first_name = \"Jane\", last_name = \"Doe\", confirmation = 12345)\n",
    "# get_flight_info(get_key_fn(know_base))\n",
    "\n",
    "(get_key | get_flight_info).invoke(know_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://9e62c61988f86e7a88.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9e62c61988f86e7a88.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://9e62c61988f86e7a88.gradio.live\n",
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableMap,       ## Wrap an implicit \"dictionary\" runnable\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, ChatMessage, AIMessage\n",
    "from typing import Iterable\n",
    "import gradio as gr\n",
    "\n",
    "external_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a SkyFlow chatbot, and you are helping a customer with their issue. Please chat with them! Stay concise and clear!\"\n",
    "        \" Your running knowledge base is: {know_base}. This is for you only; Do not mention it! \\nUsing that, we retrieved the following: {context}\\n\"\n",
    "        \" If they provide info and the retrieval fails, ask to confirm their first/last name and confirmation. Do not ask them any other personal info.\"\n",
    "        \" If it's not important to know about their flight, do not ask. The checking happens automatically; you cannot check manually.\"\n",
    "    )),\n",
    "    (\"assistant\", \"{output}\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "##########################################################################\n",
    "## Knowledge Base Things\n",
    "\n",
    "class KnowledgeBase(BaseModel):\n",
    "    first_name: str = Field('unknown', description=\"Chatting user's first name, `unknown` if unknown\")\n",
    "    last_name: str = Field('unknown', description=\"Chatting user's last name, `unknown` if unknown\")\n",
    "    confirmation: Optional[int] = Field(None, description=\"Flight Confirmation Number, `-1` if unknown\")\n",
    "    discussion_summary: str = Field(\"\", description=\"Summary of discussion so far, including locations, issues, etc.\")\n",
    "    open_problems: str = Field(\"\", description=\"Topics that have not been resolved yet\")\n",
    "    current_goals: str = Field(\"\", description=\"Current goal for the agent to address\")\n",
    "\n",
    "parser_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', (\n",
    "        \"You are a chat assistant representing the airline SkyFlow, and are trying to track info about the conversation. \"\n",
    "        \"You have just recieved a message from the user. Please fill in the schema based on the chat. \\n{format_instructions}\"\n",
    "    )), ('user', \"{know_base} is the current state. New exchange: You said {output} and they replied {input}\")\n",
    "])\n",
    "\n",
    "fail_str = (\n",
    "    \"You cannot access user's flight/account details until they provide \"\n",
    "    \"first name, last name, and flight confirmation code.\"\n",
    ")\n",
    "\n",
    "## Your goal is to invoke the following through natural conversation\n",
    "# get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}) ->\n",
    "#     \"Jane Doe's flight from San Jose to New Orleans departs at 12:30 PM tomorrow and lands at 9:30 PM.\"\n",
    "\n",
    "## TODO: Pick a working model that you're happy with\n",
    "instruct_llm = (ChatNVIDIA(nvidia_api_key=nvidia_api_key, model=\"mixtral_8x7b\") | StrOutputParser())\n",
    "chat_model = (ChatNVIDIA(nvidia_api_key=nvidia_api_key, model=\"mixtral_8x7b\") | StrOutputParser())\n",
    "\n",
    "## The external chain is already supported, and just streams the output\n",
    "external_chain = external_prompt | chat_model\n",
    "\n",
    "## The internal chain is invoked in the loop, but is not defined yet\n",
    "\n",
    "#####################################################################################\n",
    "## START TODO: Define the extractor and internal chain to satisfy the objective\n",
    "\n",
    "extractor = RunnableAssign({'know_base' : RExtract(KnowledgeBase, instruct_llm, parser_prompt)})\n",
    "internal_chain = extractor | RunnableAssign(\n",
    "    {'context' : lambda d: itemgetter('know_base') | get_key | get_flight_info}\n",
    ")\n",
    "\n",
    "## END TODO\n",
    "#####################################################################################\n",
    "\n",
    "state = {'know_base' : KnowledgeBase()}\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=False):\n",
    "\n",
    "    ## Pulling in, updating, and printing the state\n",
    "    global state\n",
    "    state['input'] = message\n",
    "    state['history'] = history\n",
    "    state['output'] = \"\" if not history else history[-1][1]\n",
    "\n",
    "    ## Generating the new state from the internal chain\n",
    "    state = internal_chain.invoke(state)\n",
    "    print(\"State after chain run:\", state)\n",
    "\n",
    "    ## Streaming the results\n",
    "    buffer = \"\"\n",
    "    for token in external_chain.stream(state):\n",
    "        buffer += token\n",
    "        yield buffer\n",
    "\n",
    "chatbot = gr.Chatbot(value = [[None, \"Hello! I'm your SkyFlow agent! How can I help you?\"]])\n",
    "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "try:\n",
    "    ## NOTE: This should also give you a temporary public link which can be\n",
    "    ## used to access this info on the public web while the session is live.\n",
    "    demo.launch(debug=True, share=True, show_api=False)\n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
