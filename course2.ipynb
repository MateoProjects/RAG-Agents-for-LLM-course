{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#76b900\">LangChain Expression Language</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain Expression Language (LCEL) is a feature of LangChain, a tool for building applications using language models. It provides a declarative way to compose chains of tasks or processes, which are sequences of operations involving language models and other components\n",
    "\n",
    "**Runnable** is an object that wraps a function. Allow dictionaries to be implicitly converted to Runnables and let a pipe **|** operator create a Runnable that passes data from the left to the right (i.e. fn1 | fn2 is a Runnable), and you have a simple way to specify complex logic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('nvidia_api_key.json', 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Accede a la clave \"api\" en los datos cargados\n",
    "nvidia_api_key = json_data[\"api\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "\n",
      "Welcome Home!\n",
      "1: Welcome Home!\n",
      "2: Welcome Home!\n",
      "\n",
      "Output: Welcome Home!\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from functools import partial\n",
    "\n",
    "################################################################################\n",
    "## Very simple \"take input and return it\"\n",
    "identity = RunnableLambda(lambda x: x)  ## Or RunnablePassthrough works\n",
    "\n",
    "################################################################################\n",
    "## Given an arbitrary function, you can make a runnable with it\n",
    "def print_and_return(x, preface=\"\"):\n",
    "    print(f\"{preface}{x}\")\n",
    "    return x\n",
    "\n",
    "rprint0 = RunnableLambda(print_and_return)\n",
    "\n",
    "################################################################################\n",
    "## You can also pre-fill some of values using functools.partial\n",
    "rprint1 = RunnableLambda(partial(print_and_return, preface=\"1: \"))\n",
    "\n",
    "################################################################################\n",
    "## And you can use the same idea to make your own custom Runnable generator\n",
    "def RPrint(preface=\"\"):\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "################################################################################\n",
    "## Chaining two runnables\n",
    "chain1 = identity | rprint0\n",
    "chain1.invoke(\"Hello World!\")\n",
    "print()\n",
    "\n",
    "################################################################################\n",
    "## Chaining that one in as well\n",
    "output = (\n",
    "    chain1\n",
    "    | rprint1\n",
    "    | RPrint(\"2: \")\n",
    ").invoke(\"Welcome Home!\")\n",
    "\n",
    "print(\"\\nOutput:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Passing dictionaries helps us keep track of our variables by name.**\n",
    "\n",
    "Since dictionaries allow us to propagate named variables (values referenced by keys), using them is great for locking in our chain components' outputs and expectations.\n",
    "\n",
    "**LangChain prompts expect dictionaries of values.**\n",
    "\n",
    "It's quite intuitive to specify an LLM Chain in LCEL to take in a dictionary and produce a string, and equally easy to raise said string back up to be a dictionary. This is very intentional and is partially due to the above reason. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example 1:** A Simple LLM Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh my beak! Birds are so sleek,\n",
      "Their feathers so bright, their songs so meek.\n",
      "They flit and they flutter, they chirp and they tweet,\n",
      "Their beauty is something to make your heart beat.\n",
      "\n",
      "They come in so many colors, shapes, and sizes,\n",
      "From the tiny hummingbird to the majestic eagle's cries.\n",
      "They soar through the skies, they dive and they play,\n",
      "Bringing joy to our lives each and every day.\n",
      "\n",
      "They build their nests with care and with love,\n",
      "Laying their eggs and raising their young above.\n",
      "They forage for food, they hunt and they seek,\n",
      "Always on the lookout for their next meal to speak.\n",
      "\n",
      "So here's to the birds, oh so grand,\n",
      "A true marvel of nature, a wonder to see in this land.\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "## Simple Chat Pipeline\n",
    "chat_llm = ChatNVIDIA(nvidia_api_key=nvidia_api_key,model=\"llama2_13b\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Only respond in rhymes\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "rhyme_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "print(rhyme_chain.invoke({\"input\" : \"Tell me about birds!\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes personals**: aquí el que està passant és que primer es generar el prompt creant un diccionari fent servir la funció **ChatPromptTemplate** , després si li passa al chatllm, ja que és una cadena langchain i amb l'output que s'obté del LLM se li envia a stroutput parser per obtenir directament el resultat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If i want I can create a UI using gradio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://27add31c243875eeb3.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://27add31c243875eeb3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://27add31c243875eeb3.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "#######################################################\n",
    "## Non-streaming Interface like that shown above\n",
    "\n",
    "# def rhyme_chat(message, history):\n",
    "#     return rhyme_chain.invoke({\"input\" : message})\n",
    "\n",
    "# gr.ChatInterface(rhyme_chat).launch()\n",
    "\n",
    "#######################################################\n",
    "## Streaming Interface\n",
    "\n",
    "def rhyme_chat_stream(message, history):\n",
    "    ## This is a generator function, where each call will yield the next entry\n",
    "    buffer = \"\"\n",
    "    for token in rhyme_chain.stream({\"input\" : message}):\n",
    "        buffer += token\n",
    "        yield buffer\n",
    "\n",
    "## Uncomment when you're ready to try this. IF USING COLAB: Share=False is faster\n",
    "gr.ChatInterface(rhyme_chat_stream).queue().launch(share=True, debug=True) \n",
    "\n",
    "## NOTE: When you're done, please click the Square button (twice to be safe) to stop the session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way is actually to use the LCEL \"implicit runnable\" syntax, which allows you to use a dictionary of functions (including chains) as a runnable that runs each function and maps the value to the key in the output dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: {'input': 'Hello World'}\n",
      "B: {'input': 'Hello World'}\n",
      "C: Hello World\n",
      "D: {'word1': 'Hello', 'word2': 'World', 'words': 'Hello World'}\n",
      "E: Hello\n",
      "F: HELLO\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output': 'HELLO'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_dictionary(v, key):\n",
    "    if isinstance(v, dict):\n",
    "        return v\n",
    "    return {key : v}\n",
    "\n",
    "def RInput(key='input'):\n",
    "    '''Coercing method to mold a value (i.e. string) to in-like dict'''\n",
    "    return RunnableLambda(partial(make_dictionary, key=key))\n",
    "\n",
    "def ROutput(key='output'):\n",
    "    '''Coercing method to mold a value (i.e. string) to out-like dict'''\n",
    "    return RunnableLambda(partial(make_dictionary, key=key))\n",
    "\n",
    "################################################################################\n",
    "## Common LCEL utility for pulling values from dictionaries\n",
    "from operator import itemgetter\n",
    "\n",
    "up_and_down = (\n",
    "    RPrint(\"A: \")\n",
    "    ## Custom ensure-dictionary process\n",
    "    | RInput()\n",
    "    | RPrint(\"B: \")\n",
    "    ## Pull-values-from-dictionary utility\n",
    "    | itemgetter(\"input\")\n",
    "    | RPrint(\"C: \")\n",
    "    ## Anything-in Dictionary-out implicit map\n",
    "    | {\n",
    "        'word1' : (lambda x : x.split()[0]),\n",
    "        'word2' : (lambda x : x.split()[1]),\n",
    "        'words' : (lambda x: x),  ## <- == to RunnablePassthrough()\n",
    "    }\n",
    "    | RPrint(\"D: \")\n",
    "    | itemgetter(\"word1\")\n",
    "    | RPrint(\"E: \")\n",
    "    ## Anything-in anything-out lambda application\n",
    "    | RunnableLambda(lambda x: x.upper())\n",
    "    | RPrint(\"F: \")\n",
    "    ## Custom ensure-dictionary process\n",
    "    | ROutput()\n",
    ")\n",
    "\n",
    "up_and_down.invoke({\"input\" : \"Hello World\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from copy import deepcopy\n",
    "\n",
    "inst_llm = ChatNVIDIA(model=\"mixtral_8x7b\")  ## Feel free to change the models\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Only respond in rhymes\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "prompt2 =  ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"Only responding in rhyme, change the topic of the input poem to be about {topic}!\"\n",
    "        \" Make it happy! Try to keep the same sentence structure, but make sure it's easy to recite!\"\n",
    "        \" Try not to rhyme a word with itself.\"\n",
    "    )),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "## These are the main chains, constructed here as modules of functionality.\n",
    "chain1 = prompt1 | inst_llm | StrOutputParser()  ## only expects input\n",
    "chain2 = prompt2 | inst_llm | StrOutputParser()  ## expects both input and topic\n",
    "\n",
    "################################################################################\n",
    "## SUMMARY OF TASK: chain1 currently gets invoked for the first input.\n",
    "##  Please invoke chain2 for subsequent invocations.\n",
    "\n",
    "def rhyme_chat2_stream(message, history, return_buffer=True):\n",
    "    '''This is a generator function, where each call will yield the next entry'''\n",
    "\n",
    "    first_poem = None\n",
    "    for entry in history:\n",
    "        if entry[0] and entry[1]:\n",
    "            ## If a generation occurred as a direct result of a user input,\n",
    "            ##  keep that response (the first poem generated) and break out\n",
    "            first_poem = entry[1]\n",
    "            break\n",
    "\n",
    "    if first_poem is None:\n",
    "        ## First Case: There is no initial poem generated. Better make one up!\n",
    "\n",
    "        buffer = \"Oh! I can make a wonderful poem about that! Let me think!\\n\\n\"\n",
    "        yield buffer\n",
    "\n",
    "        ## iterate over stream generator for first generation\n",
    "        inst_out = \"\"\n",
    "        chat_gen = chain1.stream({\"input\" : message})\n",
    "        for token in chat_gen:\n",
    "            inst_out += token\n",
    "            buffer += token\n",
    "            yield buffer if return_buffer else token\n",
    "\n",
    "        passage = \"\\n\\nNow let me rewrite it with a different focus! What should the new focus be?\"\n",
    "        buffer += passage\n",
    "        yield buffer if return_buffer else passage\n",
    "\n",
    "    else:\n",
    "        ## Subsequent Cases: There is a poem to start with. Generate a similar one with a new topic!\n",
    "        buffer = f\"Sure! Here you go!\\n\\n\"\n",
    "        yield buffer\n",
    "        new_topic = message\n",
    "        inst_out = \"\"\n",
    "        ## iterate over stream generator for second generation\n",
    "        chat_gen = chain2.stream({\"input\": first_poem, \"topic\": new_topic})\n",
    "        for token in chat_gen:\n",
    "            inst_out += token\n",
    "            buffer += token\n",
    "            yield buffer if return_buffer else token\n",
    "            \n",
    "        passage = \"\\n\\nThis is fun! Give me another topic!\"\n",
    "        buffer += passage\n",
    "        yield buffer if return_buffer else passage\n",
    "\n",
    "################################################################################\n",
    "## Below: This is a small-scale simulation of the gradio routine.\n",
    "\n",
    "def queue_fake_streaming_gradio(chat_stream, history = [], max_questions=3):\n",
    "\n",
    "    ## Mimic of the gradio initialization routine, where a set of starter messages can be printed off\n",
    "    for human_msg, agent_msg in history:\n",
    "        if human_msg: print(\"\\n[ Human ]:\", human_msg)\n",
    "        if agent_msg: print(\"\\n[ Agent ]:\", agent_msg)\n",
    "\n",
    "    ## Mimic of the gradio loop with an initial message from the agent.\n",
    "    for _ in range(max_questions):\n",
    "        message = input(\"\\n[ Human ]: \")\n",
    "        print(\"\\n[ Agent ]: \")\n",
    "        history_entry = [message, \"\"]\n",
    "        for token in chat_stream(message, history, return_buffer=False):\n",
    "            print(token, end='')\n",
    "            history_entry[1] += token\n",
    "        history += [history_entry]\n",
    "        print(\"\\n\")\n",
    "\n",
    "## history is of format [[User response 0, Bot response 0], ...]\n",
    "history = [[None, \"Let me help you make a poem! What would you like for me to write?\"]]\n",
    "\n",
    "## Simulating the queueing of a streaming gradio interface, using python input\n",
    "queue_fake_streaming_gradio(\n",
    "    chat_stream = rhyme_chat2_stream,\n",
    "    history = history\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
