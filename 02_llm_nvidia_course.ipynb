{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38ee3921-2244-4545-b0df-0b0ebebff32d",
   "metadata": {
    "id": "38ee3921-2244-4545-b0df-0b0ebebff32d"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zTeQZUUuG1u1",
   "metadata": {
    "id": "zTeQZUUuG1u1"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 2:** LLM Services and AI Foundation Models</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "In this notebook, we will explore LLM services! We'll discuss the reasons for and against deploying LLMs on edge devices alongside ways to deliver powerful models to end users through scalable server deployments like those accessible through the NVIDIA AI Foundation Endpoints.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Understanding the pros and cons of running LLM services locally vs in a scalable cloud environment.\n",
    "- Getting familiar with the AI Foundation Model Endpoint schemes, including:\n",
    "    - The raw low-level connection interface facilitated by packages like `curl` and `requests`\n",
    "    - The abstractions created to make this interface function seamlessly with open-sourced software like LangChain.\n",
    "- Getting comfortable with retrieving LLM generations from the pool of endpoints and being able to select a subset of models to build your software on.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "1. What kind of model access should you give a person developing an LLM stack, and how does it compare to the access you need to provide to end-users of an AI-powered web application?\n",
    "2. When considering which devices to support, what kinds of rigid assumptions are you making about their local compute resources and what types of fallbacks should you implement?\n",
    "    - What is you wanted to deliver a jupyter labs interface with access to a private LLM deployment to customers.\n",
    "    - What if now you wanted to support their local jupyter lab environment with your private LLM deployment?\n",
    "    - Would anything have to change if you decided to support embedded devices (i.e. Jetson Nano)?\n",
    "3. **[Harder]** Assume you have Stable Diffusion, Mixtral, and Llama-13B deployed on your own compute instance in a cloud environment sharing the same GPU resource. You currently do not have a business use case for Stable Diffusion, but your teams are experimenting with the other two for LLM applications. Should you remove Stable Diffusion from your deployment?\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Notebook Source:**\n",
    "\n",
    "- This notebook is part of a larger [**NVIDIA Deep Learning Institute**](https://www.nvidia.com/en-us/training/) course titled [**Building RAG Agents with LLMs**](https://learn.next.courses.nvidia.com/courses/course-v1:DLI+S-FX-15+V1/about). If sharing this material, please give credit and link back to the original course.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc2467-fce5-4cda-800d-3b24463a32f4",
   "metadata": {
    "id": "d7dc2467-fce5-4cda-800d-3b24463a32f4"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 1**: Getting Large Models Into Your Environment\n",
    "\n",
    "Recall from the last notebook that our current environment has several microservices running on an allocated cloud instance: `jupyter-notebook-server`, `frontend`, and `milvus` (among others). We briefly discussed these, and you may have noticed that there an \"LLM Service\" was never mentioned despite the course's emphasis on LLMs for retrieval...\n",
    "\n",
    "In this course, we'll be introducing you to the [**NVIDIA AI Foundation Models and Endpoints service**](https://catalog.ngc.nvidia.com/ai-foundation-models) to use it as a legitimate prototyping LLM service with a natural path towards self-hosted large model deployment.\n",
    "\n",
    "$$---$$\n",
    "\n",
    "\n",
    "Across just about every domain, deploying massive deep learning models is a common yet challenging task. Today's models, such as Llama 2 (70B parameters) or ensemble models like Mixtral 7x8B, are products of advanced training methods, vast data resources, and powerful computing systems. Luckily for us, these models have already been trained and many use cases can already be achieved with off-the-shelf solutions. The real hurdle, however, lies in effectively hosting these models.\n",
    "\n",
    "**Deployment Scenarios for Large Models:**\n",
    "\n",
    "1. **High-End Datacenter Deployment:**\n",
    "> An uncompressed, unquantized model on a data center stack equipped with GPUs like NVIDIA's [A100](https://www.nvidia.com/en-us/data-center/a100/) or [H100](https://www.nvidia.com/en-us/data-center/a100/) to facilitate fast inference and experimentation.\n",
    "> - **Pros**: Ideal for scalable deployment and experimentation, this stack is ideal for either large training workflows or for supporting multiple users or models at the same time.  \n",
    "> - **Cons:** It is inefficient to allocate this resource for each user of your service unless the use cases involve model training/fine-tuning or interfacing with lower-level model components.\n",
    "\n",
    "2. **Modest Datacenter/Specialized Consumer Hardware Deployment:**\n",
    "> Quantized and further-optimized models can be run (one or two per instance) on more conservative datacenter GPUs such as [L40](https://www.nvidia.com/en-us/data-center/l40/)/[A30](https://www.nvidia.com/en-us/data-center/products/a30-gpu/)/[A10](https://www.nvidia.com/en-us/data-center/products/a10-gpu/) or even on some modern consumer GPUs such as the higher-VRAM [RTX 40-series GPUs](https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/).\n",
    "> - **Pros:** This setup balances inference speed with manageable limitations for single-user applications. These sessions can also be deployed on a per-user basis to run one or two large models at a time with raw access to model internals (even if they need quantization).\n",
    "> - **Cons:** Deploying an instance for each user is still costly at scale, though it may be justifiable for some niche workloads. Alternatively, assuming that users can access these resources in their local environments is likely unreasonable.\n",
    "\n",
    "3. **Consumer Hardware Deployment:**\n",
    "> Though heavily limited in ability to propagate data through a neural network, most consumer hardware does have a graphical user interface (GUI), a web browser with internet access, some amount of memory (can safely assume at least 1 GB), and a decently-powerful CPU.\n",
    "> - **Cons:** Most hardware at the moment cannot run more than one local large model at a time in any configuration, and running even one model will require significant amounts of resource management and optimizing restrictions.\n",
    "> - **Pros:** This is a reasonable and inclusive starting assumption when considering what kinds of users your services should support.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dDqGtWuYwo9",
   "metadata": {
    "id": "8dDqGtWuYwo9"
   },
   "source": [
    "In this course, your environment will be quite representative of typical consumer hardware; though we can kickstart and prototype with microservices, we are constrained by a CPU-only compute environment that will struggle to run an LLM model. While this is a significant limitation, we will still be able to take advantage of fast LLM capabilities via:\n",
    "- Access to a compute-capable service for hosting large models.\n",
    "- A streamlined interface for command input and result retrieval.\n",
    "\n",
    "With our foundation in microservices and port-based connections, we are well-positioned to explore effective interfacing options for getting LLM access for our development environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f154bced-9358-4e43-a85d-25eeef995f6a",
   "metadata": {
    "id": "f154bced-9358-4e43-a85d-25eeef995f6a"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** Hosted Large Model Services\n",
    "\n",
    "In our pursuit to provide access to Large Language Models (LLMs) in a resource-constrained environment like ours, characterized by CPU-only instances, we'll evaluate various hosting options:\n",
    "\n",
    "**Black-Box Hosted Models:**\n",
    "> Services such as [**OpenAI**](https://openai.com/) offer APIs to interact with black-box models like GPT-4. These powerful, well-integrated services can provide simple interfaces to complex pipelines that automatically track memory, call additional models, and incorporate multimodal interfaces as necessary to simplify typical use scenarios. At the same time, they maintain operational opacity and often lack a straightforward path to self-hosting.\n",
    "> - **Pros:** Easy to use out-of-the-box with shallow barriers to entry for an average user.\n",
    "> - **Cons:** Black-box deployments suffer from potential privacy concerns, limited customization, and cost implications at scale.\n",
    "\n",
    "**Self-Hosted Models:**\n",
    "\n",
    "> Behind the scenes of just about all scaled model deployments is one or more giant models running in a data center with scalable resources and lightning-fast bandwidth at their disposal. Though necessary to deploy large models at scale and maintain strong control over the provided interfaces, these systems often require expertise to set up and generally do not work well for supporting non-developer workflows for only one individual at a time. Such systems are much better for supporting many simultaneous users, multiple models, and custom interfaces.\n",
    "> - **Pros:** They offer the capability to integrate custom datasets and APIs and are primarily designed to support numerous users concurrently.\n",
    "> - **Cons:** These setups demand technical expertise to set up and are not as practical for an individual non-developer user.\n",
    "\n",
    "To get the best of both worlds, we will utilize the [**NVIDIA NGC Service**](https://www.nvidia.com/en-us/gpu-cloud/). NGC offers a suite of developer tools for designing and deploying AI solutions. Central to our needs are the [NVIDIA AI Foundation Models](https://www.nvidia.com/en-us/ai-data-science/foundation-models/), which are pre-tuned and pre-optimized models designed for easy out-of-the-box scalable deployment (as-is or with further customization). Furthermore, NGC hosts accessible model endpoints for querying live foundation models in a [scalable DGX-accelerated compute environment](https://www.nvidia.com/en-us/data-center/dgx-platform/).\n",
    "\n",
    "Currently, NGC endpoints offers a slew of complementary queries, as few as 10,000 for the largest models and much more for the smaller models. This generous quota makes NGC an ideal, virtually free resource for prototyping production-oriented systems and experimenting with new models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae66826-e829-42b4-a125-c533d2e6ffae",
   "metadata": {
    "id": "bae66826-e829-42b4-a125-c533d2e6ffae"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3:** Getting Access to AI Foundation Endpoints\n",
    "\n",
    "To get access to your endpoints, you can visit [https://catalog.ngc.nvidia.com/ai-foundation-models](https://catalog.ngc.nvidia.com/ai-foundation-models). This will take you to a screen with a selection of available models, and may look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laRA0LxghiY8",
   "metadata": {
    "id": "laRA0LxghiY8"
   },
   "source": [
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1_2qNTs5xr0AXSh6abFeJ2vrXmc9HK9gq\" width=1000px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/ai-playground-ui.png\" width=1000px/>\n",
    "<!-- > <img style=\"max-width: 1000px;\" src=\"imgs/ai-playground-ui.png\" /> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de91dfe-e28c-4858-9476-65548c3482cb",
   "metadata": {
    "id": "2de91dfe-e28c-4858-9476-65548c3482cb"
   },
   "source": [
    "You will see a selection of large models spanning a variety of use-cases including chat, instruction following, image generation, etc. Navigating to a particular model starts you off with a demo application that lets you try out the model and see what it's capable of:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XscIpqGphum_",
   "metadata": {
    "id": "XscIpqGphum_"
   },
   "source": [
    "<!-- > <img style=\"max-width: 800px;\" src=\"imgs/ai-playground-demo.png\" /> -->\n",
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1X-RNB8ouPfxEmU5Ac0vSIBQi_239E4jP\" width=1000px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/ai-playground-demo.png\" width=1000px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NP8jV9gGhyPh",
   "metadata": {
    "id": "NP8jV9gGhyPh"
   },
   "source": [
    "However, we will be primarily interested in the API tab, which will provide us with some boilerplate code to access these models in our own development environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Zbt4oxFFhwo-",
   "metadata": {
    "id": "Zbt4oxFFhwo-"
   },
   "source": [
    "<!-- > <img style=\"max-width: 800px;\" src=\"imgs/ai-playground-api.png\" /> -->\n",
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1ckAIZoy7tvtK1uNqzA9eV5RlKMbVqs1-\" width=1000px/> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/ai-playground-api.png\" width=1000px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de870e-5c0a-425b-a5f9-14868fd6dfaf",
   "metadata": {
    "id": "75de870e-5c0a-425b-a5f9-14868fd6dfaf"
   },
   "source": [
    "This will be our actual starting point, since we intend to use these models to build interesting systems throughout the course! Before we do that, we will need to generate a free NVAPI-key by signing into or creating an NGC account, which you will be prompted to do when you click the **\"Generate Key\"** button. This will give you the necessary `$API_KEY_REQUIRED_IF_EXECUTING_OUTSIDE_NGC` key with a sufficient query quota to perform extensive experimentation and develop custom solutions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f71f31-b6a1-4549-ba90-246e03b7f4d3",
   "metadata": {
    "id": "62f71f31-b6a1-4549-ba90-246e03b7f4d3"
   },
   "source": [
    "#### **Storing Your API Key**\n",
    "\n",
    "After generating your key, **please copy the key on your own storage and store it for future use!**\n",
    "\n",
    "**In The Course Environment:** We have provided a persistent storage system in the form of the `set_key` and `get_key` routes in the `docker_router` service. By allowing the microservice to store the key, all subsequent notebooks will be able to pull it from the perpetual service without requiring you to re-enter it. (It's also just a great opportunity to show how message bodies can be accepted via a port interface, as shown near the end of [`docker_router/docker_router.py`](docker_router/docker_router.py))\n",
    "\n",
    "**In Any Other Environment:** If the service isn't running or there is any problem, the try body will raise an exception and the regular \"set the current notebook runtime's environment variable from user input\" routine is invoked as a fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa4c082a-a7a8-455b-8da9-d32f6a09be77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1688,
     "status": "ok",
     "timestamp": 1702850133816,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "fa4c082a-a7a8-455b-8da9-d32f6a09be77",
    "outputId": "ae70bcec-d72c-43de-cb54-eb093cf70b05"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "NVIDIA API Key:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved NVIDIA_API_KEY beginning with \"nvapi-gOp...\"\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import requests\n",
    "import os\n",
    "\n",
    "hard_reset = False  ## <-- Set to True if you want to reset your NVIDIA_API_KEY\n",
    "while \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\") or hard_reset:\n",
    "    ## Try to set NVIDIA_API_KEY as part of docker_router routine.\n",
    "    ##  When running in course container, this helps to save your API key between sessions.\n",
    "    try: \n",
    "        response = requests.get(\"http://docker_router:8070/get_key\").json()\n",
    "        assert response.get('nvapi_key')\n",
    "    except: response = {'nvapi_key' : getpass(\"NVIDIA API Key: \")}\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = response.get(\"nvapi_key\")\n",
    "    try: requests.post(\"http://docker_router:8070/set_key/\", json={'nvapi_key' : os.environ[\"NVIDIA_API_KEY\"]}).json()\n",
    "    except: pass\n",
    "    hard_reset = False\n",
    "\n",
    "print(f\"Retrieved NVIDIA_API_KEY beginning with \\\"{os.environ.get('NVIDIA_API_KEY')[:9]}...\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7897e672-6316-472a-962a-47c547dad2a3",
   "metadata": {
    "id": "7897e672-6316-472a-962a-47c547dad2a3"
   },
   "source": [
    "Now that we have our `NVIDIA_API_KEY` loaded in to a persistent service, we'll be able to pull it in automatically in subsequent notebooks and set the environment variable accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aca2c7-df8d-48c2-baa0-8dda22d23dee",
   "metadata": {
    "id": "f9aca2c7-df8d-48c2-baa0-8dda22d23dee"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4: [Exercise]** Trying Out The AI Foundation Endpoints\n",
    "\n",
    "To start out, let's use the provided `requests` boilerplate from an LLM model's API entry. Perhaps `Llama-2-13B` or `mixtral-7bx8`is a reasonable candidate, but feel free to try your own and consider the pool of active options.\n",
    "\n",
    "To query the model, you can paste the code for the python routine in the cell below. It is much easier to use the streaming script in Python thanks to some special features of the `requests` library, so we have provided some hints to get that started for you. *Feel free to try the non-streaming way if you have the time and interest.*\n",
    "\n",
    "**NOTE:** If you would like, you can bypass this exercise by simply clicking the `Execute` button under the code block. Doing this will show what happens when you try to invoke the provided example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fff756e-2c21-4b90-a590-ad38eea277e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9402,
     "status": "ok",
     "timestamp": 1702850146697,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "1fff756e-2c21-4b90-a590-ad38eea277e9",
    "outputId": "1850eeea-1281-4fa9-dd4a-72cde0bba36b"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 28) (2586572211.py, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 28\u001b[0;36m\u001b[0m\n\u001b[0;31m    invoke_url = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/df2bee43-fb69-42b9-9ee5-f4eabbeaf3a8 \\\"\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 28)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "####################################################################################\n",
    "## HELPERS\n",
    "\n",
    "## HINT 1: The following streaming header tosses in your API key from the environment:\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {os.environ.get('NVIDIA_API_KEY')}\",\n",
    "    \"accept\": \"text/event-stream\",\n",
    "    \"content-type\": \"application/json\",\n",
    "}\n",
    "\n",
    "## HINT 2: If you're streaming, you can use print(line.decode(\"utf-8\")) for raw responses\n",
    "##  For more user-friendly responses, you may want to get_stream_token(line):\n",
    "def get_stream_token(entry: bytes):\n",
    "    \"\"\"Utility: Coerces out ['choices'][0]['delta'][content] from the bytestream\"\"\"\n",
    "    if not entry: return \"\"\n",
    "    entry = entry.decode('utf-8')\n",
    "    if entry.startswith('data: '):\n",
    "        try: entry = json.loads(entry[5:])\n",
    "        except ValueError: return \"\"\n",
    "    return entry.get('choices', [{}])[0].get('delta', {}).get('content')\n",
    "\n",
    "####################################################################################\n",
    "## TODO: Save the invocation URL for the endpoint here\n",
    "invoke_url = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/df2bee43-fb69-42b9-9ee5-f4eabbeaf3a8 \\\"\n",
    "\n",
    "## TODO: Construct the payload, which will be sent over to the endpoint\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "    {\n",
    "      \"content\": \"Write the Fibonnaci sequence in Python\",\n",
    "      \"role\": \"user\"\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.2,\n",
    "  \"top_p\": 0.7,\n",
    "  \"max_tokens\": 1024,\n",
    "  \"seed\": 42,\n",
    "  \"stream\": True\n",
    "}\n",
    "\n",
    "\n",
    "## Use requests.post to send the header (streaming meta-info) the payload to the endpoint\n",
    "## Make sure streaming is enabled, and expect the response to have an iter_lines response.\n",
    "response = requests.post(invoke_url, headers=headers, json=payload, stream=True)\n",
    "\n",
    "## If your response is an error message, this will raise an exception in Python\n",
    "response.raise_for_status()\n",
    "\n",
    "## If the post request is honored, you should be able to iterate over \n",
    "for line in response.iter_lines():\n",
    "    print(get_stream_token(line), end=\"\")\n",
    "    # if line: print(line.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142b039f-0a8b-4188-b2ea-4c30c944f328",
   "metadata": {
    "id": "142b039f-0a8b-4188-b2ea-4c30c944f328"
   },
   "source": [
    "<br>\n",
    "\n",
    "**OBSERVATIONS:**\n",
    "\n",
    "**You may notice that the chat models expect \"messages\" as input:**\n",
    "\n",
    "This may be unexpected if you're more used to raw LLM interfaces like those of local HuggingFace models, but it will look pretty standard to users of OpenAI models. By enforcing a restricted interface instead of a raw text completion one, the service can have more control over what the users can do. There are plenty of pros and cons to this interface, with some noteworthy ones below:\n",
    "- A service might restrict the use of a specific role type or parameter (i.e. system message restriction, priming message to get arbitrary generation, etc).\n",
    "- A service might enforce custom prompt formats and implement extra options under the hood that rely on the chat interface.\n",
    "- A service might use stronger assumptions to implement deeper optimizations in the inference pipeline.\n",
    "- A service might mimic another popular interface to leverage existing ecosystem compatibilities.\n",
    "\n",
    "All of these are valid reasons, and it's important to consider which interface options are best for your particular use cases when choosing or deploying your own service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f238bbe5-efb1-453f-9fa7-486b6e568bce",
   "metadata": {
    "id": "f238bbe5-efb1-453f-9fa7-486b6e568bce"
   },
   "source": [
    "**You may notice that there are two fundamental ways of querying the models:**\n",
    "\n",
    "You can **invoke without streaming**, in which case the service response will come all at once after it has been computed in full. This is great when you need the entire output of the model before doing anything else; for example, when you want to print out the whole result or use it for downstream tasks. The response body will look something like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"id\": \"d34d436a-c28b-4451-aa9c-02eed2141ed3\",\n",
    "    \"choices\": [{\n",
    "        \"index\": 0,\n",
    "        \"message\": { \"role\": \"assistant\", \"content\": \"Bonjour! ...\" },\n",
    "        \"finish_reason\": \"stop\"\n",
    "    }],\n",
    "    \"usage\": {\n",
    "        \"completion_tokens\": 450,\n",
    "        \"prompt_tokens\": 152,\n",
    "        \"total_tokens\": 602\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "You can also **invoke with streaming**, in which case the service will send out a series of requests until a final request is sent out. This is great when you can use the responses of the service as it becomes available (which is very good for language model components that print the output directly to the user as it gets generated). In this case, the response body will look a lot more like this:\n",
    "\n",
    "```json\n",
    "data:{\"id\":\"80553c6c-5243-41a7-a3ea-6d344fc9e636\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"Bon\"},\"finish_reason\":null}]}\n",
    "data:{\"id\":\"80553c6c-5243-41a7-a3ea-6d344fc9e636\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"j\"},\"finish_reason\":null}]}\n",
    "...\n",
    "data:{\"id\":\"80553c6c-5243-41a7-a3ea-6d344fc9e636\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"finish_reason\":\"stop\"}]}\n",
    "data:[DONE]\n",
    "```\n",
    "\n",
    "Both of these options can be done with relative ease using Python's `requests` library, but using the interface as-is will result in a lot of repetitive code. Luckily, we have some systems that make this significantly easier to use and incorporate into larger projects!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dcde74-8c76-4cbe-b7e1-4f7417754fc7",
   "metadata": {
    "id": "84dcde74-8c76-4cbe-b7e1-4f7417754fc7"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5:** AI Foundation Models in LangChain\n",
    "\n",
    "Despite seeming cumbersome, the manual request-oriented API provided by the endpoints is extremely useful for developing arbitrary applications. Whether your goal environment is a Jupyter environment, a node backend, or an embedded edge device, the core commands will be sufficient to construct an interface to make the service work from just about any network-accessible device – given enough time and effort. With that said, we'll need to make some connectors if we want our service to work with other frameworks.\n",
    "\n",
    "The goal of a **connector** is to convert an arbitrary API from its native core into one that a target code-base would expect. In this course, we'll want to take advantage of LangChain's thriving chain-centric ecosystem, but the raw `requests` API will not take us all the way there. Under the hood, every LangChain chat model that isn't hosted locally has to rely on such an API, but the developer-facing API is a much cleaner [`LLM` or `SimpleChatModel`-style interface](https://python.langchain.com/docs/modules/model_io/) with default parameters and some simple utility functions like `generate` and `stream`. Unfortunately, creating such a connector is well out of scope for this course and requires a decent amount of effort to get right. ***Luckily for us, it already exists!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZjYWn8d25cfg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11808,
     "status": "ok",
     "timestamp": 1702849931181,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "ZjYWn8d25cfg",
    "outputId": "be29ddd5-1e6f-4eba-99e7-3a6a7ad06992"
   },
   "outputs": [],
   "source": [
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain-nvidia-ai-endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f679641-3c7f-49eb-9475-1b3beb464981",
   "metadata": {
    "id": "8f679641-3c7f-49eb-9475-1b3beb464981"
   },
   "source": [
    "#### **ChatNVIDIA**\n",
    "\n",
    "To start off our exploration into the LangChain interface, we can start by the NVIDIA AI Foundation Endpoint LangChain model, `ChatNVIDIA`.\n",
    "\n",
    "This model is part of the LangChain extended ecosystem and can be installed locally via `pip install langchain-nvidia-aiplay`.\n",
    "\n",
    "Below, assuming we do not know what models we have at our disposal, we can query the list of models in one of two ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66slW-kBsI0",
   "metadata": {
    "id": "d66slW-kBsI0"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints._common import NVEModel  ## Backend Model\n",
    "\n",
    "## Using the backbone NVIDIA Endpoints client, which makes the calls as you saw above\n",
    "NVEModel().available_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IJeMO-peCoFa",
   "metadata": {
    "id": "IJeMO-peCoFa"
   },
   "source": [
    "In order to pull in and start using a model, you just need to:\n",
    "- Import and instantiate your `ChatNVidia` instance with a model name.\n",
    "- Then, you can choose from LangChain's supported chat model interfacing options, chief among them `.invoke`.\n",
    "\n",
    "Below, find an LLM model you would like to use and supply its name as the `model` argument to your `ChatNVIDIA` constructor. Then, feel free to query it with whatever string you'd like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea85187-f341-4846-a733-10764ea430df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3456,
     "status": "ok",
     "timestamp": 1702770968441,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "cea85187-f341-4846-a733-10764ea430df",
    "outputId": "9decf7ef-812b-4cac-fc9b-a1ffd667be36"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "## NOTE: \"playground_\" prefix is optional for our client\n",
    "chat = ChatNVIDIA(model='llama2_13b')\n",
    "chat.invoke(\"Hello! How's it going?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UgKU4scj586M",
   "metadata": {
    "id": "UgKU4scj586M"
   },
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "So... that works, but why? We imported some model called `ChatNVIDIA`, constructed it, and called it. The result was a LangChain `AIMessage`, and allegedly the response came from the AI Foundation endpount, so where is all of the information about the model urls, request structurings, and even the API key?\n",
    "\n",
    "It turns out that under the hood, a background `NVEModel` (NVIDIA Endpoint client) does all of the primitive GET and POST requests on our behalf! You can check it out and will notice that it has all the kinds of information that is necessary to make the manual call, and can even be used to do so via a method like the primitive `_get`/`_post` as well as more specialized (but still low level) `get_req_generation`/`get_req_astream`/etc. In contrast, the higher-level `ChatNVIDIA` model does a lot of default value specification and implements the components required by LangChain while relying on the `NVEModel` lower-level functionality.  \n",
    "\n",
    "Of note, both of these components are [pydantic `BaseModel`](https://docs.pydantic.dev/latest/concepts/models/) types, which is a type that will keep showing up throughout the course. Details about this construct will be brought up as necessary, but one nice thing is that instances can be interpreted as dictionaries. This means we can print out what variables belong to `ChatNVIDIA` and which ones have been moved into the `NVEModel` model. This should help to further validate that these components are involved at different abstraction levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0lKnF0nT8zUt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1702769174579,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "0lKnF0nT8zUt",
    "outputId": "ac89a2e4-ced3-4ad5-a240-3aa7c95367ac"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Dictionary comprehension of the form {key: value for key, value in dict.items()}\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m {k:v \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mchat\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m'\u001b[39m}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chat' is not defined"
     ]
    }
   ],
   "source": [
    "## Dictionary comprehension of the form {key: value for key, value in dict.items()}\n",
    "{k:v for k,v in chat if k != 'client'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TP_Rz1kS6GZO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1702769261450,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "TP_Rz1kS6GZO",
    "outputId": "632150ac-672b-4d9b-bfec-82ac06a5d798"
   },
   "outputs": [],
   "source": [
    "{k:v for k,v in chat.client if k != 'headers_tmpl'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aWLCNC3fAsVx",
   "metadata": {
    "id": "aWLCNC3fAsVx"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 6:** Wrap-Up\n",
    "\n",
    "The goal of this notebook was to provide some discussion centered around LLM service hosting strategies and introduce you to the AI Foundation Model endpoints. Along the way, we hope that you have an intuitive understanding of how remote LLM systems can be provided and accessed from edge devices!\n",
    "\n",
    "### <font color=\"#76b900\">**Great Job!**</font>\n",
    "\n",
    "### **Next Steps:**\n",
    "1. **Make sure your API key is stored somewhere where you can easily get it.**\n",
    "2. **[Optional]** Revisit the **\"Questions To Think About\" Section** at the top of the notebook and think about some possible answers.\n",
    "3. Continue on to the next video, which will talk about **LangChain and Gradio**.\n",
    "4. After the video, go on to the corresponding notebook on **LangChain and Gradio**.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ffac91-77fa-4a6c-a35a-a970fb652617",
   "metadata": {
    "id": "25ffac91-77fa-4a6c-a35a-a970fb652617"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
